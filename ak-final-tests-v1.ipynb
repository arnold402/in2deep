{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition introduction\n### TensorFlow - Help Protect the Great Barrier Reef \n\nThe Great Barrier Reef  is the world's largest coral reef system. It is larger than the Great Wall of China and the only living thing on earth visible from space. It is home to 1,500 species of fish, 400 species of corals, 130 species of sharks, rays, and a massive variety of other sea life. The marine park protects a large part of the reef from any negative impact of human use such as fishing and tourism. Unfortunately, the reef is facing one the primary threats to its system's health, that is the overpopulation of the crown-of-thorns starfish (COTS). This is a large starfish that preys upon hard, or stony coral polyps and it is one of the largest starfish in the world. An outbreak of these starfish can devastate reefs as they contribute to a loss of live coral cover. The outbreak is believed to occur in natural cycles, worsened by poor water quality and overfishing of the starfish's predators.\n\n<iframe width=\"682\" height=\"315\" src=\"https://www.youtube.com/embed/UT2noVDFoaA\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nIn order to control COTS outbreaks to ecologically sustainable levels, The Great Barrier Reef Foundation established an innovation program  in which AI technology could improve image detection underwater which is the main goal of this competition, that is, to accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.\n","metadata":{"cell_id":"e09c136c-2622-4065-b48a-da2568d03657","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# **Training YOLO Object Detection on a Kaggle Dataset**\n\n### **Overview**\n\nThis notebook walks through how to train a YOLO object detection model using the TensorFlow Object Detection API.\nIn this specific example, we'll be training an object detection model to accurately identify starfish in real-time, trained on underwater videos of coral reefs. \nEverything in this notebook is also hosted on this [GitHub repo](https://github.com/arnold402/in2deep). Our final submission is located on the main branch, while additional work is avaliable in the [development branch](https://github.com/arnold402/in2deep/tree/development). Notebook is also avaliable on Kaggle [here](https://www.kaggle.com/amyrouillard/ak-final-tests-v1/edit/run/87382593).\n\n![Great Barrier reef](https://blogger.googleusercontent.com/img/a/AVvXsEj6-rQw5r22Bt47BUTtW5bn_dcWT7zMeADwtvsAHS3kBt6w8eWTmCM649ZcJcvosIMup6flKFIaI8p4M9ZzH1yXpEaMRjvwwfVZ_hMqgXCxtwNzEK25vTa-J2ly20by3M1zx7rTymo-tBI6Fq-mj1SJfCOXsOz0Ou1Esi4h2omvQSW98AjsONsVS-EA)\n\n \n### **Our Data**\n\nWe will be using data provided by Kaggle as part of the competion. Our dataset consists of the following:\n\n#### Files\n\n**train/** - Folder containing training set photos of the form **video_{video_id}/{video_frame_number}.jpg**.\n\n**[train/test].csv** - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n\n* `sequence_frame` - The frame number within a given sequence.\n\n* `image_id` - ID code for the image, in the format '__{video_id}-{video_frame}__'\n\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in `test.csv`. A bounding box is described by the pixel coordinate (x_min, y_min) of its upper left corner within the image together with its width and height in pixels.\n\n**example_sample_submission.csv** - A sample submission file in the correct format. The actual sample submission will be provided by the API; this is only provided to illustrate how to properly format predictions. The submission format is further described on the Evaluation page.\n\n**example_test.npy** - Sample data that will be served by the example API.\n\n**greatbarrierreef** - The image delivery API that will serve the test set pixel arrays. You may need Python 3.7 and a Linux environment to run the example offline without errors.\n\n#### Time-series API Details\n\nThe API serves the images one by one, in order by video and frame number, as pixel arrays.\nExpect to see roughly 13,000 images in the test set.\nThe API will require roughly two GB of memory after initialization. The initialization step (env.iter_test()) will require meaningfully more memory than that; we recommend you do not load your model until after making that call. The API will also consume less than ten minutes of runtime for loading and serving the data. \n\nThe dataset can be found at Kaggle [here](https://www.kaggle.com/c/tensorflow-great-barrier-reef/data).\n\n\n### **Our Model**\n\nWe'll be training a YOLO neural network. For YOLO, detection is a simple regression problem which takes an input image and learns the class probabilities and bounding\nbox coordinates. Sounds simple? YOLO divides each image into a grid of S x S and each grid predicts N bounding boxes and confidence. The confidence reflects the\naccuracy of the bounding box and whether the bounding box actually contains an object(regardless of class). YOLO also predicts the classification score for each box for\nevery class in training. You can combine both the classes to calculate the probability of each class being present in a predicted box. (Consider [this](https://neptune.ai/blog/object-detection-with-yolo-hands-on-tutorial) deep dive for more!)\n\n#### YOLO compared to other detectors\n\nAlthough a convolutional neural net (CNN) is used under the hood of YOLO, itâ€™s still able to detect objects with real-time performance. Itâ€™s possible thanks to YOLOâ€™s ability to do the predictions simultaneously in a single-stage approach. Other, slower algorithms for object detection (like Faster R-CNN) typically use a two-stage approach:\n\n* in the first stage, interesting image regions are selected. These are the parts of an image that might contain any objects\n\n* in the second stage, each of these regions is classified using a convolutional neural net. \n\nUsually, there are many regions on an image with the objects. All of these regions are sent to classification. Classification is a time-consuming operation, which is why the two-stage object detection approach performs slower compared to one-stage detection.YOLO doesnâ€™t select the interesting parts of an image, thereâ€™s no need for that. Instead, it predicts bounding boxes and classes for the whole image in a single forward net pass.\n\nYolo does not face many of the limitations faced by other object detection algorithms such high computational times faced by CNN or Fast-RCNN models.\n\n### **Training**\n\nGoogle Colab provides free GPU resources. Click \"Runtime\" â†’ \"Change runtime type\" â†’ Hardware Accelerator dropdown to \"GPU.\"\nColab does have memory limitations, and notebooks must be open in your browser to run. Sessions automatically clear themselves after 12 hours.\n\n### **Inference**\n\nWe'll run inference directly in this notebook, and on test images contained in the \"test\" folder from kaggle. \n\n\n\n\n\n\n\n","metadata":{"cell_id":"05fe3b36-fc3a-4bc8-9673-b5daeefb739a","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Install required packages","metadata":{"cell_id":"2595bf98-d6b2-41a4-9d73-16276510d720","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd ..","metadata":{"cell_id":"a8dadca9-3464-4b00-b6ab-c0b869591efa","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:31:42.192049Z","iopub.execute_input":"2022-02-09T11:31:42.192698Z","iopub.status.idle":"2022-02-09T11:32:57.686114Z","shell.execute_reply.started":"2022-02-09T11:31:42.192604Z","shell.execute_reply":"2022-02-09T11:32:57.685247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{"cell_id":"00001-0d3ef0a6-799c-43e0-83c3-860050226d1d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport shutil\nimport sys\nimport time\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image, ImageDraw\nimport ast\nimport albumentations as albu\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","cell_id":"00002-3b2f10e0-2659-4526-9e7e-4a009007a6ac","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:32:57.688031Z","iopub.execute_input":"2022-02-09T11:32:57.688347Z","iopub.status.idle":"2022-02-09T11:33:01.403951Z","shell.execute_reply.started":"2022-02-09T11:32:57.688304Z","shell.execute_reply":"2022-02-09T11:33:01.403193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path to root directory\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'","metadata":{"cell_id":"00003-5b98d187-da5c-42fd-9959-adc76f194b46","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:01.405525Z","iopub.execute_input":"2022-02-09T11:33:01.405797Z","iopub.status.idle":"2022-02-09T11:33:01.409341Z","shell.execute_reply.started":"2022-02-09T11:33:01.405761Z","shell.execute_reply":"2022-02-09T11:33:01.408673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Increase Annotations\n\n**Hypothesis:** we can enlarge our training data set by adding earlier bounding boxes prior to the first detected box for each COTS.\n\nAn increase in annotations means increase in training data which hypothetically can increase accuracy.\n\n**Approach:**\n\n1. Add detections to earlier frames that have a detection in a subsequent frame as follows:\n* identify frames that have less detections than the next one\n* exclude any candidate boxes if the box has an overlap with another box in the previous frame\n* exclude the image \"margins\" to account for that some boxes may not have been visible in the frame\n* shift the bounding box by the average translation of any other matched boxes\n2. Compare some examples frames right before and right after a detection\n3. Save the results into a new training set","metadata":{"cell_id":"3e2b903e-57b3-406a-88fa-c4380672a54b","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Parameters\nIMAGE_DIM = (1280,720)\n# We will not be adding boxes to the perimeter of the image, i.e. to 5% of the image width or height\nEXCLUDE_MARGIN = 0.05","metadata":{"cell_id":"c3e37e76-ee4d-4d11-8403-a80b029d6025","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:01.411545Z","iopub.execute_input":"2022-02-09T11:33:01.412042Z","iopub.status.idle":"2022-02-09T11:33:01.419727Z","shell.execute_reply.started":"2022-02-09T11:33:01.412003Z","shell.execute_reply":"2022-02-09T11:33:01.418891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bbox(annots):\n    \"\"\"Get a list of bounding boxes coordinates and dimentions by\n    excluding key part of their dict and extracting only actual values.\n\n    Parameters\n    ----------    \n    List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            Annotations (coordinates and dimentions)\n\n    Returns\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in format [xmid, ymid, w, h] \n     \"\"\"\n\n    #extract bounding box coordinates and dimentions\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef read_data():\n    \"\"\"Safely constracts the training dataset(dataframe) from the train.csv.\n\n    Parameters\n    ----------    \n    None\n\n    Returns\n    ----------\n    dataset: Pandas Dataframe\n        Dataset in the form of a dataframe    \n    \n    \"\"\"\n    \n    #read the train csv\n    df_train = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\n    df_train['img_path'] = os.path.join('../input/tensorflow-great-barrier-reef/train_images')+\"/video_\"+df_train.video_id.astype(str)+\"/\"+df_train.video_frame.astype(str)+\".jpg\"\n    #Safely evaluate annotation encoded string containing a Python expression.\n    df_train['annotations'] = df_train['annotations'].apply(lambda x: ast.literal_eval(x))\n    df_train['bboxes'] = df_train['annotations'].apply(lambda x: get_bbox(x))\n    #get number of bounding boxes\n    df_train['Number_bbox'] = df_train['annotations'].apply(lambda x:len(x))\n    return df_train\n\n# Call read_data() to load competition data\ndf_train = read_data()","metadata":{"cell_id":"37d49f87-c6a5-40c4-a69f-f115037d3991","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:01.422395Z","iopub.execute_input":"2022-02-09T11:33:01.422748Z","iopub.status.idle":"2022-02-09T11:33:01.997277Z","shell.execute_reply.started":"2022-02-09T11:33:01.42271Z","shell.execute_reply":"2022-02-09T11:33:01.996515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add New Bounding Boxes to Previous Frames\nWe shift current annotation to previous frame and use frames that have less\nannonations than the next as they are the ones most likely to have starfish\nshow before its actually annotated.","metadata":{"cell_id":"6369b6a4-e68b-4f44-b140-b0b817ac6acb","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#shift next annotations to previous frame and rename columns to reflect that.\ndf_shift = df_train.shift(-1).rename(columns={'annotations':'annotations_n1',\n                                             'Number_bbox':'Number_bbox_n1',\n                                             'img_path':'img_path_n1'})\ndf_lagged = pd.concat([df_train, df_shift], axis=1)\n\n#identify frames that have less annotations than the next one\n#these are the candidates for adding earlier annotations\ndf_first_frames = df_lagged[df_lagged.Number_bbox < df_lagged.Number_bbox_n1]","metadata":{"cell_id":"7bdd164f-ddc0-4b3c-9db8-dbf91a4d3a5d","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:01.998764Z","iopub.execute_input":"2022-02-09T11:33:01.99902Z","iopub.status.idle":"2022-02-09T11:33:02.02991Z","shell.execute_reply.started":"2022-02-09T11:33:01.998986Z","shell.execute_reply":"2022-02-09T11:33:02.02918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_first_frames.head()","metadata":{"cell_id":"9f5f7ca7-ff12-4cdc-a58a-c4fff626705e","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:02.031242Z","iopub.execute_input":"2022-02-09T11:33:02.031665Z","iopub.status.idle":"2022-02-09T11:33:02.069988Z","shell.execute_reply.started":"2022-02-09T11:33:02.031627Z","shell.execute_reply":"2022-02-09T11:33:02.069324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def intersects(rectangle_a, rectangle_b):\n    '''Checks for intersection of two rectangles specified as [(x1,y1),(x2,y2)]\n\n    Parameters\n    ----------    \n    rectangle_a: List eg [(x1,y1),(x2,y2)]\n            List of tuples containing coordinates. \n\n    rectangle_b: List eg [(x1,y1),(x2,y2)]\n            List of tuples containing coordinates. \n\n    Returns\n    ----------\n    Boolean: True or False\n        Returns True if there is intersection and False if there isn't\n    '''\n    if(rectangle_a[1][0]<rectangle_b[0][0] or rectangle_a[1][1]<rectangle_b[0][1]):\n        return False\n    elif(rectangle_a[0][0]>rectangle_b[1][0] or rectangle_a[0][1]>rectangle_b[1][1]):\n        return False\n    else:\n        return True\n        \ndef new_bboxes(prev_bboxes, next_bboxes):\n    '''Returns the bounding boxes that are deemed new in the next frame by checking \n    the centers of the bounding box in the next frame are not contained in\n    one of the previous frame bounding boxes.\n    \n    Parameters\n    ----------\n\n    prev_bboxes: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            Annotations from the previous frame\n    next_bboxes: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            Annotations from the next frame\n    \n    Returns\n    ----------\n    new_annots: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            New annotations generated  \n    \n    '''\n    new_bbs =[]\n    delta_xs = [0]\n    delta_ys = [0]\n    delta_ws = [0]\n    delta_hs = [0]\n    for bb in next_bboxes:\n        found = False\n        for prev_bb in prev_bboxes:\n            if intersects([(bb['x'],bb['y']),(bb['x'] + bb['width'],bb['y'] + bb['height'])],\n                         [(prev_bb['x'], prev_bb['y']), (prev_bb['x'] + prev_bb['width'], \n                                                         prev_bb['y'] + prev_bb['height'])]\n                         ):\n                delta_xs.append(bb['x']-prev_bb['x'])\n                delta_ys.append(bb['y']-prev_bb['y'])\n                delta_ws.append(bb['width']-prev_bb['width'])\n                delta_hs.append(bb['height']-prev_bb['height'])\n                found = True\n                break\n        if found == False:\n            #exclude margins\n            if (bb['x'] > IMAGE_DIM[0]*EXCLUDE_MARGIN) & \\\n            (bb['x'] < (IMAGE_DIM[0]-IMAGE_DIM[0]*EXCLUDE_MARGIN)) & \\\n            (bb['y'] > IMAGE_DIM[1]*EXCLUDE_MARGIN) & \\\n            (bb['y'] < (IMAGE_DIM[1]-IMAGE_DIM[1]*EXCLUDE_MARGIN)):\n                new_bb = {'x': bb['x'], 'y': bb['y'], 'width':bb['width'], 'height':bb['height']}\n                new_bbs.append(new_bb)\n                \n    #adjust bounding boxes for avergage drift\n    for b in new_bbs:        \n        delta_x_avg = sum(delta_xs)/len(delta_xs)\n        delta_y_avg = sum(delta_ys)/len(delta_ys)\n        delta_w_avg = sum(delta_ws)/len(delta_ws)\n        delta_h_avg = sum(delta_hs)/len(delta_hs)\n        b['x'] = b['x'] + delta_x_avg\n        b['y'] = b['y'] + delta_y_avg\n               \n    return new_bbs","metadata":{"cell_id":"f58088e8-df12-4218-8ccb-2abf925fb6fd","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:02.071334Z","iopub.execute_input":"2022-02-09T11:33:02.071901Z","iopub.status.idle":"2022-02-09T11:33:02.08775Z","shell.execute_reply.started":"2022-02-09T11:33:02.071862Z","shell.execute_reply":"2022-02-09T11:33:02.087017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new column to store only bounding boxes that are new on the next frame.\ndf_first_frames['new_annotations'] = df_first_frames.apply(lambda x: \n                                                            new_bboxes(x['annotations'],\n                                                                      x['annotations_n1']),\n                                                          axis=1)","metadata":{"cell_id":"0f608c23-6b6b-45a9-ac2a-6611da9eb916","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:02.089114Z","iopub.execute_input":"2022-02-09T11:33:02.089441Z","iopub.status.idle":"2022-02-09T11:33:02.118857Z","shell.execute_reply.started":"2022-02-09T11:33:02.089404Z","shell.execute_reply":"2022-02-09T11:33:02.117212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize new bounding boxes\ndef viz_new_boxes(prev_path, next_path, prev_annots, next_annots, new_annots):  \n    \"\"\"Draws actual/original bboxes(red) and potential new annotations(yellow)\n\n    Parameters\n    ----------\n    prev_path: String [image]\n            Path of the previous frame\n    next_path: String [image]\n            Path of the next frame\n    prev_annots: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            Annotations from the previous frame\n    next_annots: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            Annotations from the next frame\n    new_annots: List of dicts eg.([{'x': 559, 'y': 213, 'width': 50, 'height': 32}])\n            New annotations generated\n    \n    \"\"\"\n    #previuos frame\n    print(prev_path)\n    img = Image.open(prev_path)\n    \n    #draw red box for the previous annotations\n    for box in prev_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n\n    #draw yellow box for the new annotations\n    for box in new_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"yellow\", width=3)\n    # Display the image\n    display(img)    \n    \n    #next frame\n    print(next_path)\n    img = Image.open(next_path)\n    #On the next frame draw next annotations as red boxes\n    for box in next_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n    # Display the image\n    display(img)","metadata":{"cell_id":"efc19659-54e6-4037-bec0-24b26c7c05ed","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:02.122869Z","iopub.execute_input":"2022-02-09T11:33:02.123722Z","iopub.status.idle":"2022-02-09T11:33:02.133698Z","shell.execute_reply.started":"2022-02-09T11:33:02.12367Z","shell.execute_reply":"2022-02-09T11:33:02.132963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get a sample of 10 consecutive frames and apply visualization\nfor index, row in df_first_frames.sample(10, random_state=12).iterrows():\n    viz_new_boxes(row.img_path,\n                  row.img_path_n1,\n                  row.annotations,\n                  row.annotations_n1,\n                  row.new_annotations)","metadata":{"cell_id":"7055e476-35cf-4aee-9f57-3526480a98b5","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:02.135139Z","iopub.execute_input":"2022-02-09T11:33:02.135502Z","iopub.status.idle":"2022-02-09T11:33:10.074368Z","shell.execute_reply.started":"2022-02-09T11:33:02.135451Z","shell.execute_reply":"2022-02-09T11:33:10.073533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the new box previously maked as yellow(new) show on the next frame as red(original) which means they are not new on that specific frame. This means we have succefully shifted/generated new bounding boxes.","metadata":{"cell_id":"968618f2-4fbd-4c0d-b68e-ce026712c1ae","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#Get new annotation column to merge with original dataset\ndf_first_frames_strip = df_first_frames[['new_annotations']]\ndf_first_frames_strip.head(2)","metadata":{"cell_id":"cea90e09-6976-4473-b971-5cd0e6feea1f","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.075925Z","iopub.execute_input":"2022-02-09T11:33:10.076199Z","iopub.status.idle":"2022-02-09T11:33:10.087937Z","shell.execute_reply.started":"2022-02-09T11:33:10.076166Z","shell.execute_reply":"2022-02-09T11:33:10.087355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Join new annotations with original dataset\ndf_train_new = df_train.join(df_first_frames_strip)\ndf_train_new.head(2)","metadata":{"cell_id":"de27ffb3-c2d6-4f48-9700-11becd13d3e4","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.089194Z","iopub.execute_input":"2022-02-09T11:33:10.089741Z","iopub.status.idle":"2022-02-09T11:33:10.114635Z","shell.execute_reply.started":"2022-02-09T11:33:10.089703Z","shell.execute_reply":"2022-02-09T11:33:10.114054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replace NaN with empty list [] as in the original dataset\ndf_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()] = df_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()].apply(lambda x: []) ","metadata":{"cell_id":"1974b4ab-a7f0-4e97-9135-b260e98a207b","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.11617Z","iopub.execute_input":"2022-02-09T11:33:10.116577Z","iopub.status.idle":"2022-02-09T11:33:10.140801Z","shell.execute_reply.started":"2022-02-09T11:33:10.116543Z","shell.execute_reply":"2022-02-09T11:33:10.13998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merge original annotations with new annotations.\ndf_train_new['merge_annotations'] = df_train_new.apply(lambda x: (x['annotations'] + x['new_annotations']),axis=1)","metadata":{"cell_id":"3f72043c-091d-49ff-8d56-61ae733cc4f5","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.142237Z","iopub.execute_input":"2022-02-09T11:33:10.14251Z","iopub.status.idle":"2022-02-09T11:33:10.666981Z","shell.execute_reply.started":"2022-02-09T11:33:10.142475Z","shell.execute_reply":"2022-02-09T11:33:10.666198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_new.head(2)","metadata":{"cell_id":"4a398b78-e36f-47c2-9997-942ea4d40070","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.668397Z","iopub.execute_input":"2022-02-09T11:33:10.668722Z","iopub.status.idle":"2022-02-09T11:33:10.683348Z","shell.execute_reply.started":"2022-02-09T11:33:10.668682Z","shell.execute_reply":"2022-02-09T11:33:10.682682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get total number of original annotations\n(df_train_new['annotations'].apply(lambda x:len(x))).sum()","metadata":{"cell_id":"d61cb899-ebd6-4cb0-b380-9ad1ddb7ddf1","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.684778Z","iopub.execute_input":"2022-02-09T11:33:10.685272Z","iopub.status.idle":"2022-02-09T11:33:10.708403Z","shell.execute_reply.started":"2022-02-09T11:33:10.685236Z","shell.execute_reply":"2022-02-09T11:33:10.707617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get total number of new annotations\n(df_train_new['new_annotations'].apply(lambda x:len(x))).sum()","metadata":{"cell_id":"6c50f211-654b-4e32-b6b8-aff99998aa29","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.709924Z","iopub.execute_input":"2022-02-09T11:33:10.710195Z","iopub.status.idle":"2022-02-09T11:33:10.731995Z","shell.execute_reply.started":"2022-02-09T11:33:10.71016Z","shell.execute_reply":"2022-02-09T11:33:10.731271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get total number of annotations combined\n(df_train_new['merge_annotations'].apply(lambda x:len(x))).sum()","metadata":{"cell_id":"b5609db6-ec87-45cb-8595-9e325a943a26","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.733717Z","iopub.execute_input":"2022-02-09T11:33:10.734142Z","iopub.status.idle":"2022-02-09T11:33:10.755618Z","shell.execute_reply.started":"2022-02-09T11:33:10.734097Z","shell.execute_reply":"2022-02-09T11:33:10.754864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of new annotations per row\ndf_train_new['New_number_bbox'] = df_train_new['merge_annotations'].apply(lambda x:len(x))","metadata":{"cell_id":"98b6f3de-bab6-4ada-b19a-12b266b89774","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.756926Z","iopub.execute_input":"2022-02-09T11:33:10.757247Z","iopub.status.idle":"2022-02-09T11:33:10.776879Z","shell.execute_reply.started":"2022-02-09T11:33:10.75721Z","shell.execute_reply":"2022-02-09T11:33:10.776232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Short report of the entire process\nprev_box_count = df_train_new['Number_bbox'].sum()\ncurr_box_count = df_train_new['New_number_bbox'].sum()\nprev_frames_with_box_count = df_train_new[df_train_new.Number_bbox >0]['video_id'].count()\ncurr_frames_with_box_count = df_train_new[df_train_new.New_number_bbox >0]['video_id'].count()\ndata = (df_train.Number_bbox>0).value_counts()/len(df_train)*100\nprint(\"Previous number of bounding boxes: \", prev_box_count)\nprint(\"New number of boxes: \", curr_box_count)\nprint(\"Number of boxes increase: \", curr_box_count-prev_box_count)\nprint(\"Previous number of frames with boxes: \", prev_frames_with_box_count)\nprint(\"New number of frames with boxes: \", curr_frames_with_box_count)\nprint(\"Number of frames with boxes increase: \", curr_frames_with_box_count-prev_frames_with_box_count)\nprint(\"Number of images with and without starfish:\\n\", f\"With BBox: {data[1]:0.2f}% | No BBox: {data[0]:0.2f}%\")","metadata":{"cell_id":"c7c1a980-bb15-49cf-9480-5ba3a12ec802","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.778142Z","iopub.execute_input":"2022-02-09T11:33:10.778871Z","iopub.status.idle":"2022-02-09T11:33:10.800622Z","shell.execute_reply.started":"2022-02-09T11:33:10.778831Z","shell.execute_reply":"2022-02-09T11:33:10.799926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an increase in the number of annotations, however there is an opportunity to get more\nannotations if we shift more frames backwards. [MACITATA](https://www.kaggle.com/bartmaciszewski) (on kaggle) also suggests \"Using 'optical flow' techniques to determine where the boxes may have come from better, fit tighter boxes, and project boxes into margins if possible\". For now we will conclude.","metadata":{"cell_id":"3047ecd3-7fe5-43a9-952f-6f5bc5ba17ff","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#Replace original annotations column with merge_annotations\ndf_train['annotations'] = df_train_new['merge_annotations']","metadata":{"cell_id":"5de09101-f706-4dcd-8321-0558359cc762","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.80197Z","iopub.execute_input":"2022-02-09T11:33:10.802578Z","iopub.status.idle":"2022-02-09T11:33:10.807799Z","shell.execute_reply.started":"2022-02-09T11:33:10.80254Z","shell.execute_reply":"2022-02-09T11:33:10.806944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”¨ Helper","metadata":{"cell_id":"00035-0da6f041-5392-46d0-97c2-e04f380bfbd5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Bounding box formatting helpers\n\nThe following image illustrates the the positions of the different points on the bounding box which relate to the different bounding box formats.\n<div style=\"text-align: center;\">\n    <img src=\"https://i.ibb.co/LQL8C7j/bounding-boxes.png\" style=\"display: block; \n           margin-left: auto;\n           margin-right: auto;\n           width: 40%;\"/>\n</div>","metadata":{"cell_id":"3c6eb783-9d0b-4872-9c05-5846825ebb48","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"Converts bounding boxes from 'voc' (Visual Object Classes) formatting to 'yolo' (You Only Look Once) formatting.\n    The voc formatting for bounding boxes is [xmin, ymin, xmax, ymax], where (xmin,ymin) and (xmax,ymax) are \n    the pixel coordinates of the top-left and bottom-right corners of a bounding box, respectively. \n    The yolo formating is [xmid, ymid, w, h] where (xmid, ymid) is the position of the center of the bounding box,\n    in coordiates normalised over the image size. w and h represent the normalised width and height of the image. \n    The default image size of the images in this training data is 1280x720 pixels.\n    \n    Parameters\n    ----------\n    bboxes: array_like, shape = [n,4]\n        Bounding boxes in voc format [xmin, ymin, xmax, ymax]\n    image_height: int, default 720\n        Height of images (Pixel height)\n    image_width: int, default 1280\n        Width of images (Pixel width)\n        \n    Returns\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in yolo format [xmid, ymid, w, h]\n    \"\"\"\n    bboxes = bboxes.copy().astype(float) \n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0] \n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"Converts bounding boxes from 'yolo' (You Only Look Once) formatting to 'coco' (Common Pbjects in Context) formatting.\n    The yolo formating is [xmid, ymid, w, h] where (xmid, ymid) is the position of the center of the bounding box,\n    in coordiates normalised over the image size. w and h represent the normalised width and height of the image. \n    The coco formatting for bounding boxes is [xmin, ymin, w, h], where (xmin,ymin) is the pixel coordinate of \n    the top-left corner of a bounding box.  w and h represent the width and height of the image in number of pixels.\n    The default image size of the images in this training data is 1280x720 pixels.\n    \n    Parameters\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in yolo format [xmid, ymid, w, h]\n    image_height: int, default=720\n        Height of images (Pixel height)\n    image_width: int, default=1280\n        Width of images (Pixel width)\n        \n    Returns\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in coco format [xmin, ymin, w, h] \n    \"\"\"\n    bboxes = bboxes.copy().astype(float)\n    \n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"Converts bounding boxes from 'voc' (Visual Object Classes) formatting to 'coco' (Common objects in Context) formatting.\n    The voc formatting for bounding boxes is [xmin, ymin, xmax, ymax], where (xmin,ymin) and (xmax,ymax) are \n    the pixel coordinates of the top-left and bottom-right corners of a bounding box, respectively. \n    he coco formatting for bounding boxes is [xmin, ymin, w, h], where (xmin,ymin) is the pixel coordinate of \n    the top-left corner of a bounding box.  w and h represent the width and height of the image in number of pixels. \n    The default image size of the images in this training data is 1280x720 pixels.\n    \n    Parameters\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in voc format [xmin, ymin, xmax, ymax]\n    image_height: int, default 720\n        Height of images (Pixel height)\n    image_width: int, default 1280\n        Width of images (Pixel width)\n        \n    Returns\n    ----------\n    bboxes: array, shape = [n,4]\n        Bounding boxes in coco format [xmin, ymin, w, h]\n    \"\"\"\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n","metadata":{"_kg_hide-input":true,"cell_id":"00036-b6424a31-1644-4944-a2a6-f7651f462264","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.810905Z","iopub.execute_input":"2022-02-09T11:33:10.811102Z","iopub.status.idle":"2022-02-09T11:33:10.825216Z","shell.execute_reply.started":"2022-02-09T11:33:10.811078Z","shell.execute_reply":"2022-02-09T11:33:10.824421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting helpers\nThe following functions can be used to plot the bounding boxes overlaying them on the original image.","metadata":{"cell_id":"47290b15-60a2-40f5-98da-5a602581be47","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"#Randomly select a color for bounding box plots\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255)) for idx in range(1)]\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    \"\"\" Plots one bounding box on image img using cv2.rectangle.\n    The color, label and line thickness of the rectangular box can also be customised.\n    \n    Parameters\n    ----------\n    x: list, shape = [4]\n     Bounding box in voc format [xmin, ymin, xmax, ymax]\n    img: PIL\n        Image, for example, cv2.imread(image_path)\n    color: list, shape = [3], default=None\n        Color of rectangular bounding box\n    label: string, default=None\n        Class label of bounding box\n    line_thickness: float, default=None\n        Line thinkness of bounding box\n    \"\"\"\n    \n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  \n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA) \n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n        \n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n    \"\"\" Plots bounding boxes on image img.\n    Bounding box input format should be correctly specified. The color, label and line thickness of boxes can also be customised.\n    \n    Parameters\n    ----------\n    img: PIL\n        Image, for example, cv2.imread(image_path)\n    bboxes: array, shape=[n,4]\n        Bounding boxes\n    classes: list of strings, shape=[n]\n        Name of classe associated with each bounding box\n    class_ids: list of ints, shape=[n]\n        Id of class associated with each bounding box\n    color: list, shape = [3], default=None\n        Color of rectangular bounding box\n    show_classes: list of strings, shape=[n], default=None\n        List of alternative class names\n    bbox_format: string, default='yolo'\n        Format of bounding boxes, options are 'yolo', 'coco' and 'voc_pascal'\n    class_name: boolian, default=False\n        Chose to show class name or id.\n    line_thickness: float, default=None\n        Line thinkness of bounding box\n        \n    Return\n    ----------\n    image: PIL\n        Image overlayed with labeled bounding boxes.\n    \"\"\"\n    \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    \"\"\"Show image with overlayed plots of bounding boxes of all starfish detected.\n    \n    Parameters\n    ----------\n    img: PIL\n        Original image of reef.\n    bboxes: array, shape=[n,4]\n        Bounding boxes of detected starfish.\n    bbox_format: string, default='yolo'\n        Format of bounding boxes, options are 'yolo', 'coco' and 'voc_pascal'\n        \n    Returns\n    ----------\n    Image: PIL\n        Image overlayed with bounding boxes\n    \"\"\"\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"cell_id":"cf5d3d87-b34c-4831-876b-0f76b345298c","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.826954Z","iopub.execute_input":"2022-02-09T11:33:10.827673Z","iopub.status.idle":"2022-02-09T11:33:10.855689Z","shell.execute_reply.started":"2022-02-09T11:33:10.827633Z","shell.execute_reply":"2022-02-09T11:33:10.854852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tracker helpers\nThe following function is a tracking helper which convert detected objects as predicted by the yolov5 model into detections compatible with the norfair library. The norfair library assits with identifying starfish based on detections from a previous image frame. ","metadata":{"cell_id":"167bad8a-15f8-458d-be04-0c8dc4820520","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Import norfair library\nfrom norfair import Detection, Tracker\n\ndef detection2norfair(detects, frame_id):\n    \"\"\"Convert prediction in the form of [xmin, ymin, xmax, ymax, score] to norfair.Detection class.\n    \n    Parameters\n    ----------\n    detects: list, shape=[n,5]\n        Detections made in the format [xmin, ymin, xmax, ymax, score]\n    frame_id: int\n        Image frame id\n        \n    Returns\n    ----------\n    result: list, shape=[n,3]\n        List of decections in the norfair.Detection class format.\n    \"\"\"\n    result = []\n    for xmin, ymin, xmax, ymax, score in detects:\n        xc, yc = (xmin + xmax) / 2, (ymin + ymax) / 2\n        w, h = xmax - xmin, ymax - ymin\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n","metadata":{"cell_id":"00037-5df36dab-f0cb-4242-9895-9ba19e8b752d","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.857142Z","iopub.execute_input":"2022-02-09T11:33:10.857692Z","iopub.status.idle":"2022-02-09T11:33:10.909709Z","shell.execute_reply.started":"2022-02-09T11:33:10.857653Z","shell.execute_reply":"2022-02-09T11:33:10.908947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The helper function 'euclidean_distance' is passed to the Tracker and computes the distance between detected objects in order to match detections in subsequent frames.","metadata":{"cell_id":"de4c0d42-36de-4b13-9e00-6fc1f0b03eaf","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def euclidean_distance(detection, tracked_object):\n    \"\"\"Compute the euclidean distance between two onjects.\n    Function used to match detections on this frame with tracked_objects from previous frames.\n    \n    Parameters\n    ----------\n    detections: object\n        Object detected and save in norfair.Detection class.\n    tracked_object: norfair.Tracked_object\n        Object being tracked.\n        \n    Returns\n    ----------\n    n: float\n        Euclidean datance between the detected object and the estimated position of the tracked object.\n    \"\"\"\n    n = np.linalg.norm(detection.points - tracked_object.estimate)\n    return n","metadata":{"cell_id":"505186aa-fe93-43af-809f-9a77c0f901dd","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.912097Z","iopub.execute_input":"2022-02-09T11:33:10.912679Z","iopub.status.idle":"2022-02-09T11:33:10.917931Z","shell.execute_reply.started":"2022-02-09T11:33:10.912638Z","shell.execute_reply":"2022-02-09T11:33:10.917171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Yolov5 pipline\nLoad yolov5 weights and configurations and create a prediction pipeline, including tracking of starfish over consecutive image frames. ","metadata":{"cell_id":"1742c3ce-ed2d-41be-b1d1-7d658c23b6ce","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Make configuration directory\n!mkdir -p /root/.config/Ultralytics\n# Copy yolo font to configuration directory\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\n\ndef load_model(checkpoint_path, conf=0.28, iou=0.40):\n    \"\"\"Load yolov5 model\n    \n    Parameters\n    ----------\n    checkpoint_path: string\n        Path to weights stored in pt file\n    conf: float, default=0.28\n        Confidence threshold\n    iou: float, default=0.40\n        IoU (intersection over union) threshold\n        \n    Returns\n    ----------\n    model: \n        The output of the callable model.\n    \"\"\"\n    \n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=checkpoint_path,\n                           source='local',\n                           force_reload=True)  \n    model.conf = conf  \n    model.iou  = iou \n    model.classes = None   \n    model.multi_label = False \n    model.max_det = 20\n    \n    return model\n\ndef predict(model, img, size=9000, augment=False):\n    \"\"\"Perform object detection of image img using model.\n    Predict the postion of objects in an image and return the bounding boxes\n    and corresponding confidences.\n    \n    Parameters\n    ----------\n    model: object\n        Callable yolov5 model. \n    img: PIL\n        Image on which object detection will be performed\n    size: int, default=9000\n        Image size \n    augement: bool, default=False\n    \n    Returns\n    ----------\n    bboxes: list, shape=[n,4]\n        Bounding poxes of predicted objects\n    confs: list, shape=[n]\n        Confidence in prediction\n    \"\"\"\n    \n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    \"\"\"Format predictions of bounding boxes, in 'coco' format, and confidences/scores according to\n    the kaggle competion submission format.\n    \n    Parameters\n    ----------\n    bboxes: list, shape=[n,4]\n        Bounding boxes of predicted objects in 'coco' format.\n    conf: list, shape=[n]\n        Confidence in prediction\n    \n    Returns\n    ----------\n    annot: string\n        Annotations in kaggle competion submission format, 'confidence'+' '+'xmin'+' '+'ymin'+' '+'width'+' '+'height'.\n    \"\"\"\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n","metadata":{"_kg_hide-input":true,"cell_id":"00039-2dc94ec6-4539-494d-b059-0fb828515160","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:10.919961Z","iopub.execute_input":"2022-02-09T11:33:10.921659Z","iopub.status.idle":"2022-02-09T11:33:12.309523Z","shell.execute_reply.started":"2022-02-09T11:33:10.921563Z","shell.execute_reply":"2022-02-09T11:33:12.308534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \"\"\"Uses objects loacted in previous frame to identify objects in current frame.\n    Appends list of detections with detections predicted by norfair tracker based on detections\n    made in a previous frame. Score and bounding box information is provides by tracker. \n    \n    Parameters\n    ----------\n    tracker: object\n        norfair tracker\n    frame_id: int\n        Image frame id\n    bboxes: list, shape=[n,4]\n        Bounding boxes of predicted objects in 'coco' format.\n    scores: list, shape=[n]\n        Confidence in prediction.\n    \n    Returns\n    ----------\n    predictions: list of string, shape=[n]\n        Preditions made by model appended with those predictions made by norfair tracker\n        with format 'score'+' '+'xmin'+' '+'ymin'+' '+'width'+' '+'height' (coco format).\n    \"\"\"\n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n\n    tracked_objects = tracker.update(detections=detection2norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id: \n            continue\n        \n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"cell_id":"9c01f2b9-6058-4b00-977e-b194df944b9a","tags":[],"deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:12.314653Z","iopub.execute_input":"2022-02-09T11:33:12.314886Z","iopub.status.idle":"2022-02-09T11:33:12.325314Z","shell.execute_reply.started":"2022-02-09T11:33:12.314858Z","shell.execute_reply":"2022-02-09T11:33:12.324596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”­ Inference on **Train**\n\nIn the following section the model is tested on a sample of five randomly selected images from the competition data. The predictions are plotted.","metadata":{"cell_id":"00044-5c0c1e4d-97ac-4ee9-bd8e-7eb2ace39884","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Model parameters","metadata":{"cell_id":"dcf631e5-4975-48f0-9225-0e5d4094ce42","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Path to weights\nCKPT_PATH = '../input/yolov5s6/f2_sub2.pt'\n# Image size\nIMG_SIZE  = 6400\n# Confidence threshold\nCONF      = 0.30\n# IoU threshold\nIOU       = 0.50\n# Are the images augmented\nAUGMENT   = False","metadata":{"cell_id":"00045-81aa1aaa-00bf-4e4a-9b75-e56bc0410c14","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:33:12.327019Z","iopub.execute_input":"2022-02-09T11:33:12.327562Z","iopub.status.idle":"2022-02-09T11:33:12.33688Z","shell.execute_reply.started":"2022-02-09T11:33:12.327522Z","shell.execute_reply":"2022-02-09T11:33:12.336063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n# Initialize norfair tracker fucntion\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\n# Load yolov5 model\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\n# Fetch copy of dataframe\ndf = df_train.copy()\ndf['image_path'] = '/kaggle/input/tensorflow-great-barrier-reef/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\n\n# Select a random sample of 5 consecutive images containing starfish\ndf_sample = df.iloc[1:6, :]\nwhile df_sample.Number_bbox.sum()<len(df_sample):\n    nrows = range(df.shape[0])\n    ix = random.randint(nrows.start, nrows.stop-5)\n    df_sample = df.iloc[ix:ix+5, :]\n\n# Create a list of the paths to the selected images\nimage_paths = df_sample.image_path.tolist()\n\n# Predict starfish locations and plot side-by-side with actual starfish locations\nframe_id = df_sample.iloc[0].video_frame\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n\n    # Predict bounding boxes and confidences\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    # Use norfair tracker to add predictions based on previous image frames\n    predict_box = tracking_function(tracker, frame_id, bboxes, confis)\n    \n    # Split predict_box strings and convert string to int\n    if len(predict_box)>0:\n        box = [list(map(int,box.split(' ')[1:])) for box in predict_box]\n    else:\n        box = []\n    \n    # Plot results\n    fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n    fig.tight_layout()\n    bboxes_true = df_sample[(df_sample['image_path']==path)]['bboxes'].to_numpy()\n    axs[0].imshow(show_img(img, bboxes_true[0], bbox_format='coco'))\n    axs[0].set_title('True location of COTS')\n    axs[1].imshow(show_img(img, box, bbox_format='coco'))\n    axs[1].set_title('Predicted loaction of COTS')\n    \n    # Break if more than 5 images are sampled\n    if idx>5:\n        break\n    frame_id += 1","metadata":{"_kg_hide-input":true,"cell_id":"00047-cc49d224-1526-42d4-b3a9-0787eb3d9918","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:36:29.325296Z","iopub.execute_input":"2022-02-09T11:36:29.325597Z","iopub.status.idle":"2022-02-09T11:36:35.351888Z","shell.execute_reply.started":"2022-02-09T11:36:29.325563Z","shell.execute_reply":"2022-02-09T11:36:35.348683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ† Submission","metadata":{"cell_id":"0269c36c-90bf-4347-b6fe-6481065d4175","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Initalize `Env`","metadata":{"cell_id":"00048-8314beb6-95cf-4961-a9aa-0505902ef495","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Import competition library\nimport greatbarrierreef\n# Initialize the environment\nenv = greatbarrierreef.make_env()\n# Iterator which loops over the test set and sample submission\niter_test = env.iter_test()      \n\n# Change to working directory on Kaggle\n%cd ../working","metadata":{"_kg_hide-input":true,"cell_id":"00049-2d6f099e-1c8b-4f5a-8197-c6b610314420","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:34:39.831746Z","iopub.execute_input":"2022-02-09T11:34:39.83209Z","iopub.status.idle":"2022-02-09T11:34:39.859425Z","shell.execute_reply.started":"2022-02-09T11:34:39.832049Z","shell.execute_reply":"2022-02-09T11:34:39.858694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ”­ Run Inference on **Test**","metadata":{"cell_id":"00051-bb901ee4-4cca-42a8-a555-982b60654b5d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Assume all frames are consecutive and add one to frame_id after each iteration\nframe_id =0\n# Loop over iter_test iterator\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    # Predict bounding boxes and confidences\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    # Use norfair tracker to add predictions based on previous image frames\n    predictions = tracking_function(tracker, frame_id, bboxes, confs)\n    \n    # Join predictions strings in a single string\n    prediction_str = ' '.join(predictions)\n    # Add predictions to pred_df dataframe\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n    # Show the first images\n    if frame_id < 3:\n        if len(predict_box)>0:\n            box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n        else:\n            box = []\n        display(show_img(img, box, bbox_format='coco'))\n        \n    frame_id += 1","metadata":{"_kg_hide-input":true,"cell_id":"00052-f32ebcd2-a3a2-41ba-88d5-091a4cfce1f2","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:34:42.785526Z","iopub.execute_input":"2022-02-09T11:34:42.786301Z","iopub.status.idle":"2022-02-09T11:34:44.72688Z","shell.execute_reply.started":"2022-02-09T11:34:42.786251Z","shell.execute_reply":"2022-02-09T11:34:44.726007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ  Check Submission","metadata":{"cell_id":"00053-456ca7e8-63ba-435e-874a-ea56589c4480","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"cell_id":"00054-5fd5860c-b0be-4530-b6e1-98a0cdb230a7","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2022-02-09T11:36:49.340742Z","iopub.execute_input":"2022-02-09T11:36:49.341386Z","iopub.status.idle":"2022-02-09T11:36:49.353124Z","shell.execute_reply.started":"2022-02-09T11:36:49.341344Z","shell.execute_reply":"2022-02-09T11:36:49.352143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ‘€References\n\n* [More Annotations!](https://www.kaggle.com/bartmaciszewski/more-annotations)\n* [Only yolov5 + tracking](https://www.kaggle.com/kocha1/only-yolov5-tracking-lb-642)\n* [Yolov5 infer](https://www.kaggle.com/freshair1996/leon-v5-infer-2-0)\n* Zhang, Minghua, et al. \"Lightweight Underwater Object Detection Based on YOLO v4 and Multi-Scale Attentional Feature Fusion.\" Remote Sensing 13.22 (2021): 4706.","metadata":{"cell_id":"eb9cc61c-6d3c-4ac2-9a66-396e9cca290a","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e93dcea3-c9f3-481b-b801-91715891ffa5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}]}