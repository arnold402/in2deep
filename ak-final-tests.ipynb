{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:36:24.192076Z","iopub.execute_input":"2022-02-09T05:36:24.192561Z","iopub.status.idle":"2022-02-09T05:37:39.738531Z","shell.execute_reply.started":"2022-02-09T05:36:24.192465Z","shell.execute_reply":"2022-02-09T05:37:39.737703Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nimport time\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image, ImageDraw\nimport ast\nimport albumentations as albu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T05:37:39.740629Z","iopub.execute_input":"2022-02-09T05:37:39.740958Z","iopub.status.idle":"2022-02-09T05:37:43.303759Z","shell.execute_reply.started":"2022-02-09T05:37:39.740915Z","shell.execute_reply":"2022-02-09T05:37:43.302984Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:37:43.305260Z","iopub.execute_input":"2022-02-09T05:37:43.305534Z","iopub.status.idle":"2022-02-09T05:37:43.311082Z","shell.execute_reply.started":"2022-02-09T05:37:43.305499Z","shell.execute_reply":"2022-02-09T05:37:43.309406Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Increase Annotations\n\n**Hypothesis:** we can enlarge our training data set by adding earlier bounding boxes prior to the first detected box for each COTS\n\nAn increase in annotations means increase in the target variable which hypotheticaly can increase accuracy.\n\n**Approach:**\n\n1. Add detections to earlier frames that have a detection in a subsequent frame as follows:\n* identify frames that have less detections than the next one\n* exclude any candidate boxes if the box has an overlap with another box in the previous frame\n* exclude the image \"margins\" to account for that some boxes may not have been visible in the frame\n* shift the bounding box by the average translation of any other matched boxes\n2. Compare some examples frames right before and right after a detection\n3. Save the results into a new training set","metadata":{}},{"cell_type":"code","source":"#Parameters\nIMAGE_DIM = (1280,720)\n#we will not be adding boxes in 5% of the image..\n#..width or height\nEXCLUDE_MARGIN = 0.05","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:37:43.313251Z","iopub.execute_input":"2022-02-09T05:37:43.313661Z","iopub.status.idle":"2022-02-09T05:37:43.320407Z","shell.execute_reply.started":"2022-02-09T05:37:43.313626Z","shell.execute_reply":"2022-02-09T05:37:43.319685Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Data Extraction Helper functions","metadata":{}},{"cell_type":"code","source":"def get_bbox(annots):\n    \"\"\"Get a list of bounding boxes by excluding key part \n    of their dict and extracting only actual values.\"\"\"\n    #extract bounding box coordinates and dimentions\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef read_data():\n    \"\"\"Get all the data required/provided.\"\"\"\n    \n    #read the train csv\n    df_train = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\n    df_train['img_path'] = os.path.join('../input/tensorflow-great-barrier-reef/train_images')+\"/video_\"+df_train.video_id.astype(str)+\"/\"+df_train.video_frame.astype(str)+\".jpg\"\n    #Safely evaluate annotation encoded string containing a Python expression.\n    df_train['annotations'] = df_train['annotations'].apply(lambda x: ast.literal_eval(x))\n    df_train['bboxes'] = df_train['annotations'].apply(lambda x: get_bbox(x))\n    #get number of bounding boxes\n    df_train['Number_bbox'] = df_train['annotations'].apply(lambda x:len(x))\n    return df_train\n\ndf_train = read_data()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:38:30.784447Z","iopub.execute_input":"2022-02-09T05:38:30.785221Z","iopub.status.idle":"2022-02-09T05:38:31.374330Z","shell.execute_reply.started":"2022-02-09T05:38:30.785165Z","shell.execute_reply":"2022-02-09T05:38:31.373578Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Add New Bounding Boxes to Previous Frames\nWe shift current annotation to previous frame and use frames that have less\nannonations than the next as they are the ones most likely to have starfish\nshow before its actually annotated.","metadata":{}},{"cell_type":"code","source":"#shift next annotations to previous frame and rename columns to reflect that.\ndf_shift = df_train.shift(-1).rename(columns={'annotations':'annotations_n1',\n                                             'Number_bbox':'Number_bbox_n1',\n                                             'img_path':'img_path_n1'})\ndf_lagged = pd.concat([df_train, df_shift], axis=1)\n\n#identify frames that have less annotations than the next one\n#these are the candidates for adding earlier annotations\ndf_first_frames = df_lagged[df_lagged.Number_bbox < df_lagged.Number_bbox_n1]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:38:41.557182Z","iopub.execute_input":"2022-02-09T05:38:41.557897Z","iopub.status.idle":"2022-02-09T05:38:41.588171Z","shell.execute_reply.started":"2022-02-09T05:38:41.557857Z","shell.execute_reply":"2022-02-09T05:38:41.587452Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_first_frames.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:38:42.718797Z","iopub.execute_input":"2022-02-09T05:38:42.719299Z","iopub.status.idle":"2022-02-09T05:38:42.760067Z","shell.execute_reply.started":"2022-02-09T05:38:42.719264Z","shell.execute_reply":"2022-02-09T05:38:42.759395Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def intersects(rectangle_a, rectangle_b):\n    '''Checks for intersection of two rectangles specified as [(x1,y1),(x2,y2)]'''\n    if(rectangle_a[1][0]<rectangle_b[0][0] or rectangle_a[1][1]<rectangle_b[0][1]):\n        return False\n    elif(rectangle_a[0][0]>rectangle_b[1][0] or rectangle_a[0][1]>rectangle_b[1][1]):\n        return False\n    else:\n        return True\n        \ndef new_bboxes(prev_bboxes, next_bboxes):\n    '''Returns the bounding boxes that are deemed new in the next frame by checking \n    the centers of the bounding box in the next frame are not contained in\n    one of the previous frame bounding boxes.'''\n    new_bbs =[]\n    delta_xs = [0]\n    delta_ys = [0]\n    delta_ws = [0]\n    delta_hs = [0]\n    for bb in next_bboxes:\n        found = False\n        for prev_bb in prev_bboxes:\n            if intersects([(bb['x'],bb['y']),(bb['x'] + bb['width'],bb['y'] + bb['height'])],\n                         [(prev_bb['x'], prev_bb['y']), (prev_bb['x'] + prev_bb['width'], \n                                                         prev_bb['y'] + prev_bb['height'])]\n                         ):\n                delta_xs.append(bb['x']-prev_bb['x'])\n                delta_ys.append(bb['y']-prev_bb['y'])\n                delta_ws.append(bb['width']-prev_bb['width'])\n                delta_hs.append(bb['height']-prev_bb['height'])\n                found = True\n                break\n        if found == False:\n            #exclude margins\n            if (bb['x'] > IMAGE_DIM[0]*EXCLUDE_MARGIN) & \\\n            (bb['x'] < (IMAGE_DIM[0]-IMAGE_DIM[0]*EXCLUDE_MARGIN)) & \\\n            (bb['y'] > IMAGE_DIM[1]*EXCLUDE_MARGIN) & \\\n            (bb['y'] < (IMAGE_DIM[1]-IMAGE_DIM[1]*EXCLUDE_MARGIN)):\n                new_bb = {'x': bb['x'], 'y': bb['y'], 'width':bb['width'], 'height':bb['height']}\n                new_bbs.append(new_bb)\n                \n    #adjust bounding boxes for avergage drift\n    for b in new_bbs:        \n        delta_x_avg = sum(delta_xs)/len(delta_xs)\n        delta_y_avg = sum(delta_ys)/len(delta_ys)\n        delta_w_avg = sum(delta_ws)/len(delta_ws)\n        delta_h_avg = sum(delta_hs)/len(delta_hs)\n        b['x'] = b['x'] + delta_x_avg\n        b['y'] = b['y'] + delta_y_avg\n               \n    return new_bbs","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:46:01.745368Z","iopub.execute_input":"2022-02-09T05:46:01.745888Z","iopub.status.idle":"2022-02-09T05:46:01.761285Z","shell.execute_reply.started":"2022-02-09T05:46:01.745851Z","shell.execute_reply":"2022-02-09T05:46:01.760400Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# create new column to store only bounding boxes that are new on the next frame.\ndf_first_frames['new_annotations'] = df_first_frames.apply(lambda x: \n                                                            new_bboxes(x['annotations'],\n                                                                      x['annotations_n1']),\n                                                          axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:46:04.098879Z","iopub.execute_input":"2022-02-09T05:46:04.099477Z","iopub.status.idle":"2022-02-09T05:46:04.125798Z","shell.execute_reply.started":"2022-02-09T05:46:04.099437Z","shell.execute_reply":"2022-02-09T05:46:04.125034Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Visualize new bounding boxes\ndef viz_new_boxes(prev_path, next_path, prev_annots, next_annots, new_annots):  \n    \"\"\"Draws actual/original bboxes(red) and potential new annotations(yellow)\"\"\"\n    #previuos frame\n    print(prev_path)\n    img = Image.open(prev_path)\n    \n    #draw red box for the previous annotations\n    for box in prev_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n\n    #draw yellow box for the new annotations\n    for box in new_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"yellow\", width=3)\n\n    display(img)    \n    \n    #next frame\n    print(next_path)\n    img = Image.open(next_path)\n    #On the next frame draw red boxes this include boxes previously denoted as yellow\n    for box in next_annots:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n        \n    display(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:50:18.644422Z","iopub.execute_input":"2022-02-09T05:50:18.645007Z","iopub.status.idle":"2022-02-09T05:50:18.655433Z","shell.execute_reply.started":"2022-02-09T05:50:18.644964Z","shell.execute_reply":"2022-02-09T05:50:18.654428Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Get a sample of 10 consecutive frames and apply visualization\nfor index, row in df_first_frames.sample(10, random_state=12).iterrows():\n    viz_new_boxes(row.img_path,\n                  row.img_path_n1,\n                  row.annotations,\n                  row.annotations_n1,\n                  row.new_annotations)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T05:52:14.066212Z","iopub.execute_input":"2022-02-09T05:52:14.066819Z","iopub.status.idle":"2022-02-09T05:52:22.436627Z","shell.execute_reply.started":"2022-02-09T05:52:14.066775Z","shell.execute_reply":"2022-02-09T05:52:22.435672Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"All the new box previously maked as yellow show on the next frame as red which means they are not new on that specific frame. This means we have succefully shifted/generated new bounding boxes.","metadata":{}},{"cell_type":"code","source":"#Get new annotation column to merge with original dataset\ndf_first_frames_strip = df_first_frames[['new_annotations']]\ndf_first_frames_strip.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:02:18.043135Z","iopub.execute_input":"2022-02-09T06:02:18.043411Z","iopub.status.idle":"2022-02-09T06:02:18.057495Z","shell.execute_reply.started":"2022-02-09T06:02:18.043379Z","shell.execute_reply":"2022-02-09T06:02:18.056411Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Join new annotations with original dataset\ndf_train_new = df_train.join(df_first_frames_strip)\ndf_train_new.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:05.497461Z","iopub.execute_input":"2022-02-09T06:09:05.497730Z","iopub.status.idle":"2022-02-09T06:09:05.522433Z","shell.execute_reply.started":"2022-02-09T06:09:05.497701Z","shell.execute_reply":"2022-02-09T06:09:05.521671Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Replace NaN with empty list [] as in the original dataset\ndf_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()] = df_train_new['new_annotations'].loc[df_train_new['new_annotations'].isnull()].apply(lambda x: []) ","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:06.007796Z","iopub.execute_input":"2022-02-09T06:09:06.008599Z","iopub.status.idle":"2022-02-09T06:09:06.028934Z","shell.execute_reply.started":"2022-02-09T06:09:06.008549Z","shell.execute_reply":"2022-02-09T06:09:06.027911Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Merge original annotations with new annotations.\ndf_train_new['merge_annotations'] = df_train_new.apply(lambda x: (x['annotations'] + x['new_annotations']),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:09.221626Z","iopub.execute_input":"2022-02-09T06:09:09.222189Z","iopub.status.idle":"2022-02-09T06:09:09.768626Z","shell.execute_reply.started":"2022-02-09T06:09:09.222149Z","shell.execute_reply":"2022-02-09T06:09:09.767884Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train_new.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:09.770269Z","iopub.execute_input":"2022-02-09T06:09:09.770613Z","iopub.status.idle":"2022-02-09T06:09:09.785706Z","shell.execute_reply.started":"2022-02-09T06:09:09.770573Z","shell.execute_reply":"2022-02-09T06:09:09.784880Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Get total number of original annotations\n(df_train_new['annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:13.067078Z","iopub.execute_input":"2022-02-09T06:09:13.067644Z","iopub.status.idle":"2022-02-09T06:09:13.086627Z","shell.execute_reply.started":"2022-02-09T06:09:13.067604Z","shell.execute_reply":"2022-02-09T06:09:13.085715Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Get total number of new annotations\n(df_train_new['new_annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:14.168711Z","iopub.execute_input":"2022-02-09T06:09:14.169381Z","iopub.status.idle":"2022-02-09T06:09:14.189859Z","shell.execute_reply.started":"2022-02-09T06:09:14.169214Z","shell.execute_reply":"2022-02-09T06:09:14.188945Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Get total number of annotations combined\n(df_train_new['merge_annotations'].apply(lambda x:len(x))).sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:09:16.162835Z","iopub.execute_input":"2022-02-09T06:09:16.163412Z","iopub.status.idle":"2022-02-09T06:09:16.182607Z","shell.execute_reply.started":"2022-02-09T06:09:16.163372Z","shell.execute_reply":"2022-02-09T06:09:16.181729Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Number of new annotations per row\ndf_train_new['New_number_bbox'] = df_train_new['merge_annotations'].apply(lambda x:len(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:12:32.655329Z","iopub.execute_input":"2022-02-09T06:12:32.655952Z","iopub.status.idle":"2022-02-09T06:12:32.677132Z","shell.execute_reply.started":"2022-02-09T06:12:32.655911Z","shell.execute_reply":"2022-02-09T06:12:32.676355Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Short report of the entire process\nprev_box_count = df_train_new['Number_bbox'].sum()\ncurr_box_count = df_train_new['New_number_bbox'].sum()\nprev_frames_with_box_count = df_train_new[df_train_new.Number_bbox >0]['video_id'].count()\ncurr_frames_with_box_count = df_train_new[df_train_new.New_number_bbox >0]['video_id'].count()\nprint(\"Previous number of bounding boxes: \", prev_box_count)\nprint(\"New number of boxes: \", curr_box_count)\nprint(\"Number of boxes increase: \", curr_box_count-prev_box_count)\nprint(\"Previous number of frames with boxes: \", prev_frames_with_box_count)\nprint(\"New number of frames with boxes: \", curr_frames_with_box_count)\nprint(\"Number of frames with boxes increase: \", curr_frames_with_box_count-prev_frames_with_box_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:13:08.683903Z","iopub.execute_input":"2022-02-09T06:13:08.684167Z","iopub.status.idle":"2022-02-09T06:13:08.702824Z","shell.execute_reply.started":"2022-02-09T06:13:08.684136Z","shell.execute_reply":"2022-02-09T06:13:08.702096Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"There is an increase in the number of annotations. However there is an opportunity to get more\nannotations if we shift more frames backwards. [MACITATA](https://www.kaggle.com/bartmaciszewski) (on kaggle) also suggests \"Using 'optical flow' techniques to determine where the boxes may have come from better, fit tighter boxes, and project boxes into margins if possible\". For now we will conclude.","metadata":{}},{"cell_type":"code","source":"#Replace original annotations column with merge_annotations\ndf_train['annotations'] = df_train_new['merge_annotations']","metadata":{"execution":{"iopub.status.busy":"2022-02-09T06:25:15.649780Z","iopub.execute_input":"2022-02-09T06:25:15.650521Z","iopub.status.idle":"2022-02-09T06:25:15.655848Z","shell.execute_reply.started":"2022-02-09T06:25:15.650473Z","shell.execute_reply":"2022-02-09T06:25:15.655105Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Get a copy of train dataset and prepare for ML\ndf = df_train.copy()\ndf['image_path'] = f'/kaggle/input/tensorflow-great-barrier-reef/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\n#df['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.127626Z","iopub.execute_input":"2022-02-08T16:32:59.128038Z","iopub.status.idle":"2022-02-08T16:32:59.215214Z","shell.execute_reply.started":"2022-02-08T16:32:59.127998Z","shell.execute_reply":"2022-02-08T16:32:59.214488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FDA_reference = df[df['annotations']!='[]']","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.566952Z","iopub.execute_input":"2022-02-08T16:32:59.567756Z","iopub.status.idle":"2022-02-08T16:32:59.582687Z","shell.execute_reply.started":"2022-02-08T16:32:59.567707Z","shell.execute_reply":"2022-02-08T16:32:59.581742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FDA_trans = albu.FDA(FDA_reference['image_path'].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:32:59.869292Z","iopub.execute_input":"2022-02-08T16:32:59.871459Z","iopub.status.idle":"2022-02-08T16:32:59.875696Z","shell.execute_reply.started":"2022-02-08T16:32:59.87142Z","shell.execute_reply":"2022-02-08T16:32:59.874989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:01.286982Z","iopub.execute_input":"2022-02-08T16:33:01.287558Z","iopub.status.idle":"2022-02-08T16:33:01.393937Z","shell.execute_reply.started":"2022-02-08T16:33:01.287518Z","shell.execute_reply":"2022-02-08T16:33:01.393081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”¨ Helper","metadata":{}},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:01.775752Z","iopub.execute_input":"2022-02-08T16:33:01.776328Z","iopub.status.idle":"2022-02-08T16:33:01.813107Z","shell.execute_reply.started":"2022-02-08T16:33:01.776293Z","shell.execute_reply":"2022-02-08T16:33:01.812316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##############################################################\n#                      Tracking helpers                      #\n##############################################################\n\nimport numpy as np\nfrom norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:02.00805Z","iopub.execute_input":"2022-02-08T16:33:02.008659Z","iopub.status.idle":"2022-02-08T16:33:02.061867Z","shell.execute_reply.started":"2022-02-08T16:33:02.008621Z","shell.execute_reply":"2022-02-08T16:33:02.061209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:02.247881Z","iopub.execute_input":"2022-02-08T16:33:02.248441Z","iopub.status.idle":"2022-02-08T16:33:03.636276Z","shell.execute_reply.started":"2022-02-08T16:33:02.248404Z","shell.execute_reply":"2022-02-08T16:33:03.635256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_model(ckpt_path, conf=0.28, iou=0.40):\ndef load_model(ckpt_path, conf=0.28, iou=0.40):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 20  # maximum number of detections per image\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:03.638273Z","iopub.execute_input":"2022-02-08T16:33:03.638744Z","iopub.status.idle":"2022-02-08T16:33:03.647735Z","shell.execute_reply.started":"2022-02-08T16:33:03.638707Z","shell.execute_reply":"2022-02-08T16:33:03.646936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”­ Inference","metadata":{}},{"cell_type":"markdown","source":"## Helper","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=9000, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:03.64925Z","iopub.execute_input":"2022-02-08T16:33:03.650059Z","iopub.status.idle":"2022-02-08T16:33:03.661763Z","shell.execute_reply.started":"2022-02-08T16:33:03.650019Z","shell.execute_reply":"2022-02-08T16:33:03.661033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:03.999388Z","iopub.execute_input":"2022-02-08T16:33:04.000046Z","iopub.status.idle":"2022-02-08T16:33:04.011106Z","shell.execute_reply.started":"2022-02-08T16:33:04.000004Z","shell.execute_reply":"2022-02-08T16:33:04.008379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Train**","metadata":{}},{"cell_type":"code","source":"CKPT_PATH = '../input/yolov5s6/f2_sub2.pt'\nIMG_SIZE  = 6400\nCONF      = 0.30\nIOU       = 0.50\nAUGMENT   = False\nFDA_aug = False","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:04.627212Z","iopub.execute_input":"2022-02-08T16:33:04.627483Z","iopub.status.idle":"2022-02-08T16:33:04.63184Z","shell.execute_reply.started":"2022-02-08T16:33:04.627454Z","shell.execute_reply":"2022-02-08T16:33:04.630703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CLAHE(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    equalized = clahe.apply(gray)\n    return equalized\ndef Gamma_enhancement(image):\n    gamma = 1/0.6\n    R = 255.0\n    return (R * np.power(image.astype(np.uint32)/R, gamma)).astype(np.uint8)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:04.907808Z","iopub.execute_input":"2022-02-08T16:33:04.908512Z","iopub.status.idle":"2022-02-08T16:33:04.915844Z","shell.execute_reply.started":"2022-02-08T16:33:04.908477Z","shell.execute_reply":"2022-02-08T16:33:04.915187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nimage_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nframe_id = 0\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    if FDA_aug:\n        img = FDA_trans(image=img)['image']\n    bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    predict_box = tracking_function(tracker, frame_id, bboxes, confis)\n\n    if len(predict_box)>0:\n        box = [list(map(int,box.split(' ')[1:])) for box in predict_box]\n    else:\n        box = []\n    display(show_img(img, box, bbox_format='coco'))\n    if idx>5:\n        break\n    frame_id += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:05.206981Z","iopub.execute_input":"2022-02-08T16:33:05.207976Z","iopub.status.idle":"2022-02-08T16:33:18.903581Z","shell.execute_reply.started":"2022-02-08T16:33:05.207925Z","shell.execute_reply":"2022-02-08T16:33:18.902803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init `Env`","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:18.905742Z","iopub.execute_input":"2022-02-08T16:33:18.906205Z","iopub.status.idle":"2022-02-08T16:33:18.931136Z","shell.execute_reply.started":"2022-02-08T16:33:18.906168Z","shell.execute_reply":"2022-02-08T16:33:18.930399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ../working","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:18.932479Z","iopub.execute_input":"2022-02-08T16:33:18.932762Z","iopub.status.idle":"2022-02-08T16:33:18.938784Z","shell.execute_reply.started":"2022-02-08T16:33:18.932715Z","shell.execute_reply":"2022-02-08T16:33:18.93797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**","metadata":{}},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nframe_id =0\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    if FDA_aug:\n        img = FDA_trans(image=img)['image']\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n\n    predictions = tracking_function(tracker, frame_id, bboxes, confs)\n    \n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n    if frame_id < 3:\n        if len(predict_box)>0:\n            box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n        else:\n            box = []\n        display(show_img(img, box, bbox_format='coco'))\n#     print('Prediction:', pred_df)\n    frame_id += 1\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-08T16:33:18.941179Z","iopub.execute_input":"2022-02-08T16:33:18.941704Z","iopub.status.idle":"2022-02-08T16:33:21.240929Z","shell.execute_reply.started":"2022-02-08T16:33:18.94164Z","shell.execute_reply":"2022-02-08T16:33:21.24004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ‘€ Check Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:33:21.242638Z","iopub.execute_input":"2022-02-08T16:33:21.243095Z","iopub.status.idle":"2022-02-08T16:33:21.256456Z","shell.execute_reply.started":"2022-02-08T16:33:21.243059Z","shell.execute_reply":"2022-02-08T16:33:21.255652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}